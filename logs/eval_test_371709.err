Map:   0%|          | 0/164 [00:00<?, ? examples/s]Map: 100%|██████████| 164/164 [00:00<00:00, 1172.34 examples/s]
Map:   0%|          | 0/164 [00:00<?, ? examples/s]Map: 100%|██████████| 164/164 [00:00<00:00, 3941.63 examples/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/net/scratch/jingyang/ShorterBetter/eval/Coding/human_eval/evaluate_human_eval.py", line 113, in <module>
[rank0]:     completions = generate_sample_batch(problems["instruction"])
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/net/scratch/jingyang/ShorterBetter/eval/Coding/human_eval/evaluate_human_eval.py", line 34, in generate_sample_batch
[rank0]:     llm = LLM(
[rank0]:           ^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/utils.py", line 1039, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 240, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 482, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 271, in __init__
[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 49, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 40, in _init_executor
[rank0]:     self.collective_rpc("load_model")
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 49, in collective_rpc
[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/utils.py", line 2208, in run_method
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/worker/worker.py", line 182, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1113, in load_model
[rank0]:     self.model = get_model(vllm_config=self.vllm_config)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 12, in get_model
[rank0]:     return loader.load_model(vllm_config=vllm_config)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 377, in load_model
[rank0]:     model = _initialize_model(vllm_config=vllm_config)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 119, in _initialize_model
[rank0]:     return model_class(vllm_config=vllm_config, prefix=prefix)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 451, in __init__
[rank0]:     self.model = Qwen2Model(vllm_config=vllm_config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 149, in __init__
[rank0]:     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 305, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:                                                     ^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 556, in make_layers
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 307, in <lambda>
[rank0]:     lambda prefix: Qwen2DecoderLayer(config=config,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 218, in __init__
[rank0]:     self.mlp = Qwen2MLP(
[rank0]:                ^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 80, in __init__
[rank0]:     self.down_proj = RowParallelLinear(
[rank0]:                      ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1052, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 127, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jingyang22/.conda/envs/rp/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 142.81 MiB is free. Including non-PyTorch memory, this process has 44.19 GiB memory in use. Of the allocated memory 43.88 GiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W211 19:45:49.480316236 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/var/spool/slurmd/job371709/slurm_script: line 164: syntax error near unexpected token `done'
/var/spool/slurmd/job371709/slurm_script: line 164: `done'
